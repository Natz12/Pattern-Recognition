{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SYDE675_assignment_2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPoIKOhKEVuMrTk3IFvuGw4"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"qSaCwf1b85TQ","colab_type":"text"},"source":["# SYDE 675 Pattern Recognition\n","## Natalia Hoyos"]},{"cell_type":"code","metadata":{"id":"gDm9E72k8EZz","colab_type":"code","colab":{}},"source":["# Import the libraries\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","# import math\n","\n","import statistics # mode() function a sub-set of the statistics module\n","\n","verbose = False"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XkIXXAVBIO5C","colab_type":"code","outputId":"daaac9af-167d-44ca-c118-dd0325c1c04d","executionInfo":{"status":"ok","timestamp":1584500181804,"user_tz":240,"elapsed":1482,"user":{"displayName":"Natalia Hoyos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghcs5ZP7rKTAzT6Gwfv3E0um_onlcGkkBv1PxLVajI=s64","userId":"09348541312897718275"}},"colab":{"base_uri":"https://localhost:8080/","height":487}},"source":["# Try getting the file online if not found try it locally\n","try:\n","  print(\"Getting wine dataset file from the internet\")\n","  df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data',\n","               header = None) #Load from url\n","except:\n","  print(\"Could not find the wine dataset, getting the file locally\")\n","  df_wine = pd.read_csv('wine.data',\n","               header=None) \n","else:\n","  print(\"Wine dataset file loaded from the web\")\n","\n","y = df_wine.iloc[:,0]\n","df_wine = df_wine.iloc[:,1:]\n","df_wine[df_wine.shape[1]+1] = y\n","\n","np_wine = df_wine.values\n","\n","# Try getting the file online if not found try it locally\n","try:\n","  print(\"Getting wine dataset file from the internet\")\n","  df_ttt = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/tic-tac-toe/tic-tac-toe.data',\n","               header = None) #Load from url\n","except:\n","  print(\"Could not find the wine dataset, getting the file locally\")\n","  df_ttt = pd.read_csv('tic-tac-toe.data',\n","               header=None) \n","else:\n","  print(\"Wine dataset file loaded from the web\")\n","\n","np_ttt = df_ttt.values\n","# df_wine[50:100]\n","# array_wine\n","# df_ttt[0].unique()\n","df_wine.rename(columns = {0:'truth'})\n","\n","df_wine"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Getting wine dataset file from the internet\n","Wine dataset file loaded from the web\n","Getting wine dataset file from the internet\n","Wine dataset file loaded from the web\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>14</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>14.23</td>\n","      <td>1.71</td>\n","      <td>2.43</td>\n","      <td>15.6</td>\n","      <td>127</td>\n","      <td>2.80</td>\n","      <td>3.06</td>\n","      <td>0.28</td>\n","      <td>2.29</td>\n","      <td>5.64</td>\n","      <td>1.04</td>\n","      <td>3.92</td>\n","      <td>1065</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>13.20</td>\n","      <td>1.78</td>\n","      <td>2.14</td>\n","      <td>11.2</td>\n","      <td>100</td>\n","      <td>2.65</td>\n","      <td>2.76</td>\n","      <td>0.26</td>\n","      <td>1.28</td>\n","      <td>4.38</td>\n","      <td>1.05</td>\n","      <td>3.40</td>\n","      <td>1050</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>13.16</td>\n","      <td>2.36</td>\n","      <td>2.67</td>\n","      <td>18.6</td>\n","      <td>101</td>\n","      <td>2.80</td>\n","      <td>3.24</td>\n","      <td>0.30</td>\n","      <td>2.81</td>\n","      <td>5.68</td>\n","      <td>1.03</td>\n","      <td>3.17</td>\n","      <td>1185</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>14.37</td>\n","      <td>1.95</td>\n","      <td>2.50</td>\n","      <td>16.8</td>\n","      <td>113</td>\n","      <td>3.85</td>\n","      <td>3.49</td>\n","      <td>0.24</td>\n","      <td>2.18</td>\n","      <td>7.80</td>\n","      <td>0.86</td>\n","      <td>3.45</td>\n","      <td>1480</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>13.24</td>\n","      <td>2.59</td>\n","      <td>2.87</td>\n","      <td>21.0</td>\n","      <td>118</td>\n","      <td>2.80</td>\n","      <td>2.69</td>\n","      <td>0.39</td>\n","      <td>1.82</td>\n","      <td>4.32</td>\n","      <td>1.04</td>\n","      <td>2.93</td>\n","      <td>735</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>173</th>\n","      <td>13.71</td>\n","      <td>5.65</td>\n","      <td>2.45</td>\n","      <td>20.5</td>\n","      <td>95</td>\n","      <td>1.68</td>\n","      <td>0.61</td>\n","      <td>0.52</td>\n","      <td>1.06</td>\n","      <td>7.70</td>\n","      <td>0.64</td>\n","      <td>1.74</td>\n","      <td>740</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>174</th>\n","      <td>13.40</td>\n","      <td>3.91</td>\n","      <td>2.48</td>\n","      <td>23.0</td>\n","      <td>102</td>\n","      <td>1.80</td>\n","      <td>0.75</td>\n","      <td>0.43</td>\n","      <td>1.41</td>\n","      <td>7.30</td>\n","      <td>0.70</td>\n","      <td>1.56</td>\n","      <td>750</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>175</th>\n","      <td>13.27</td>\n","      <td>4.28</td>\n","      <td>2.26</td>\n","      <td>20.0</td>\n","      <td>120</td>\n","      <td>1.59</td>\n","      <td>0.69</td>\n","      <td>0.43</td>\n","      <td>1.35</td>\n","      <td>10.20</td>\n","      <td>0.59</td>\n","      <td>1.56</td>\n","      <td>835</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>176</th>\n","      <td>13.17</td>\n","      <td>2.59</td>\n","      <td>2.37</td>\n","      <td>20.0</td>\n","      <td>120</td>\n","      <td>1.65</td>\n","      <td>0.68</td>\n","      <td>0.53</td>\n","      <td>1.46</td>\n","      <td>9.30</td>\n","      <td>0.60</td>\n","      <td>1.62</td>\n","      <td>840</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>177</th>\n","      <td>14.13</td>\n","      <td>4.10</td>\n","      <td>2.74</td>\n","      <td>24.5</td>\n","      <td>96</td>\n","      <td>2.05</td>\n","      <td>0.76</td>\n","      <td>0.56</td>\n","      <td>1.35</td>\n","      <td>9.20</td>\n","      <td>0.61</td>\n","      <td>1.60</td>\n","      <td>560</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>178 rows × 14 columns</p>\n","</div>"],"text/plain":["        1     2     3     4    5     6   ...    9      10    11    12    13  14\n","0    14.23  1.71  2.43  15.6  127  2.80  ...  2.29   5.64  1.04  3.92  1065   1\n","1    13.20  1.78  2.14  11.2  100  2.65  ...  1.28   4.38  1.05  3.40  1050   1\n","2    13.16  2.36  2.67  18.6  101  2.80  ...  2.81   5.68  1.03  3.17  1185   1\n","3    14.37  1.95  2.50  16.8  113  3.85  ...  2.18   7.80  0.86  3.45  1480   1\n","4    13.24  2.59  2.87  21.0  118  2.80  ...  1.82   4.32  1.04  2.93   735   1\n","..     ...   ...   ...   ...  ...   ...  ...   ...    ...   ...   ...   ...  ..\n","173  13.71  5.65  2.45  20.5   95  1.68  ...  1.06   7.70  0.64  1.74   740   3\n","174  13.40  3.91  2.48  23.0  102  1.80  ...  1.41   7.30  0.70  1.56   750   3\n","175  13.27  4.28  2.26  20.0  120  1.59  ...  1.35  10.20  0.59  1.56   835   3\n","176  13.17  2.59  2.37  20.0  120  1.65  ...  1.46   9.30  0.60  1.62   840   3\n","177  14.13  4.10  2.74  24.5   96  2.05  ...  1.35   9.20  0.61  1.60   560   3\n","\n","[178 rows x 14 columns]"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"FhcmHkeWEMHJ","colab_type":"code","outputId":"7878607e-82a7-4be1-d81d-aee12e8ccb93","executionInfo":{"status":"ok","timestamp":1584500181805,"user_tz":240,"elapsed":1477,"user":{"displayName":"Natalia Hoyos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghcs5ZP7rKTAzT6Gwfv3E0um_onlcGkkBv1PxLVajI=s64","userId":"09348541312897718275"}},"colab":{"base_uri":"https://localhost:8080/","height":419}},"source":["df_ttt"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>x</td>\n","      <td>x</td>\n","      <td>x</td>\n","      <td>x</td>\n","      <td>o</td>\n","      <td>o</td>\n","      <td>x</td>\n","      <td>o</td>\n","      <td>o</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>x</td>\n","      <td>x</td>\n","      <td>x</td>\n","      <td>x</td>\n","      <td>o</td>\n","      <td>o</td>\n","      <td>o</td>\n","      <td>x</td>\n","      <td>o</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>x</td>\n","      <td>x</td>\n","      <td>x</td>\n","      <td>x</td>\n","      <td>o</td>\n","      <td>o</td>\n","      <td>o</td>\n","      <td>o</td>\n","      <td>x</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>x</td>\n","      <td>x</td>\n","      <td>x</td>\n","      <td>x</td>\n","      <td>o</td>\n","      <td>o</td>\n","      <td>o</td>\n","      <td>b</td>\n","      <td>b</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>x</td>\n","      <td>x</td>\n","      <td>x</td>\n","      <td>x</td>\n","      <td>o</td>\n","      <td>o</td>\n","      <td>b</td>\n","      <td>o</td>\n","      <td>b</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>953</th>\n","      <td>o</td>\n","      <td>x</td>\n","      <td>x</td>\n","      <td>x</td>\n","      <td>o</td>\n","      <td>o</td>\n","      <td>o</td>\n","      <td>x</td>\n","      <td>x</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>954</th>\n","      <td>o</td>\n","      <td>x</td>\n","      <td>o</td>\n","      <td>x</td>\n","      <td>x</td>\n","      <td>o</td>\n","      <td>x</td>\n","      <td>o</td>\n","      <td>x</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>955</th>\n","      <td>o</td>\n","      <td>x</td>\n","      <td>o</td>\n","      <td>x</td>\n","      <td>o</td>\n","      <td>x</td>\n","      <td>x</td>\n","      <td>o</td>\n","      <td>x</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>956</th>\n","      <td>o</td>\n","      <td>x</td>\n","      <td>o</td>\n","      <td>o</td>\n","      <td>x</td>\n","      <td>x</td>\n","      <td>x</td>\n","      <td>o</td>\n","      <td>x</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>957</th>\n","      <td>o</td>\n","      <td>o</td>\n","      <td>x</td>\n","      <td>x</td>\n","      <td>x</td>\n","      <td>o</td>\n","      <td>o</td>\n","      <td>x</td>\n","      <td>x</td>\n","      <td>negative</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>958 rows × 10 columns</p>\n","</div>"],"text/plain":["     0  1  2  3  4  5  6  7  8         9\n","0    x  x  x  x  o  o  x  o  o  positive\n","1    x  x  x  x  o  o  o  x  o  positive\n","2    x  x  x  x  o  o  o  o  x  positive\n","3    x  x  x  x  o  o  o  b  b  positive\n","4    x  x  x  x  o  o  b  o  b  positive\n","..  .. .. .. .. .. .. .. .. ..       ...\n","953  o  x  x  x  o  o  o  x  x  negative\n","954  o  x  o  x  x  o  x  o  x  negative\n","955  o  x  o  x  o  x  x  o  x  negative\n","956  o  x  o  o  x  x  x  o  x  negative\n","957  o  o  x  x  x  o  o  x  x  negative\n","\n","[958 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"4cUANo0EqEq-","colab_type":"code","colab":{}},"source":["# sns.pairplot(df_wine, hue = 0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xLAaZV__9gob","colab_type":"code","colab":{}},"source":["\n","\n","# bound_up = math.ceil(data_size/ folds)\n","# bound_low = math.floor(data_size/ folds)\n","# datasplit = []\n","# for i in range(data_size % folds - 1):\n","#   # print(i)\n","#   datasplit.append(data[i * up_bound : (i + 1) * up_bound])\n","# for i in range(folds - data_size % folds - 1):\n","#   datasplit.append()\n","\n","# df\n","# df[df.group == 7]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MDi9tC3zxF3w","colab_type":"code","colab":{}},"source":["class Tree:\n","  def __init__(self, feature = None, is_feature_categorical = None,\n","               threshold = None, categories = None, is_leaf = None, \n","               prediction = None, leaves = None):\n","    self.feature = feature\n","    self.is_feature_categorical = False\n","    self.threshold = None\n","    self.categories = None\n","    self.is_leaf = True\n","    self.prediction = None\n","    # self.left = None\n","    # self.right = None\n","    self.leaves = None\n","\n","\n","\n","# accepts numpy array\n","def train_decision_tree(data,  class_column = -1, features_ignore = [], use_IG = True):\n","\n","  \"\"\"Return the trained ID3 tree.\n","\n","  Trains a decision tree using ID3 algorithm. \n","\n","  Parameters: \n","  # X (np.array): np.array with the features of the samples in the training set\n","  # y (np.array): np.array with the labels of the samples in the training set\n","  data: np.array with the samples in the training set\n","  class_column: index of the column having the class\n","\n","  Returns: \n","  object of class Tree.\n","\n","\n","  #https://docs.python.org/3/library/doctest.html\n","  # >>> [factorial(n) for n in range(6)]\n","  # [1, 1, 2, 6, 24, 120]\n","  # >>> factorial(30)\n","  # 265252859812191058636308480000000\n","  # >>> factorial(-1)\n","  # Traceback (most recent call last):\n","  #     ...\n","  # ValueError: n must be >= 0\n","\n","  # Factorials of floats are OK, but the float must be an exact integer:\n","  # >>> factorial(30.1)\n","  # Traceback (most recent call last):\n","  #     ...\n","  # ValueError: n must be exact integer\n","  # >>> factorial(30.0)\n","  # 265252859812191058636308480000000\n","\n","  # It must also not be ridiculously large:\n","  # >>> factorial(1e100)\n","  # Traceback (most recent call last):\n","  #     ...\n","  # OverflowError: n too large\n","  \"\"\"\n","  tree_current = Tree()\n","\n","  # data = df.values\n","  if class_column == -1:\n","    class_column = data.shape[1]-1\n","  X = np.delete(data, class_column, axis=1) \n","  y = data[:, class_column]\n","  # col = df.columns\n","  features = [item for item in range(data.shape[1]) if item not in features_ignore and item != class_column]\n","  # print(\"features: \" , * features)\n","  \n","  # Test if all samples have the same label and assign the prediction to that label\n","  if (np.unique(y).shape[0] == 1):\n","    tree_current.prediction = y[0]\n","    tree_current.is_leaf = True #(I know being True is default, but here it is explicit)\n","  \n","  # Test if there are features left, if not, assign the most common (mode)\n","  elif len(features) == 0:\n","    labels, label_count = np.unique((y), return_counts=1) \n","    i = np.argmax(label_count)\n","    \n","    #https://stackoverflow.com/questions/10797819/finding-the-mode-of-a-list\n","    # tree_current.prediction = max(set(y), key=y.count)\n","    tree_current.prediction = labels[i]\n","    tree_current.is_leaf = True #(I know being True is default, but here it is explicit)\n","  \n","  # Since everything else has failed, find the next split\n","  else:\n","    tree_current.is_leaf = False\n","\n","    labels, label_count = np.unique((y), return_counts=1) \n","    i = np.argmax(label_count)\n","    tree_current.prediction = labels[i]\n","  \n","    feature_best, threshold_best, is_categorical = best_conditional_entropy(data, class_column, features, use_IG)\n","\n","    # verbose\n","    # print(is_categorical)\n","    if is_categorical:\n","      tree_current.threshold = None\n","      tree_current.categories = np.unique(data[:,feature_best])\n","\n","    else:\n","      tree_current.threshold = threshold_best\n","      tree_current.categories = None\n","\n","    tree_current.feature = feature_best\n","    \n","    tree_current.is_feature_categorical = is_categorical\n","    features_ignore = np.append(features_ignore, feature_best)\n","\n","    data_splits = data_split(data, feature_best, tree_current.threshold)\n","    leaves = []\n","    # verbose = True\n","    #verbose\n","    if verbose:\n","      print(tree_current.threshold)\n","\n","    for leaf, i in zip(data_splits, range(data_splits.shape[0])):\n","      \n","      if verbose:\n","        print('Leaf number {0:d}'.format(i))\n","        print('split attribute: ', feature_best)\n","        print('Feat ignore: ', features_ignore)\n","      leaves.append(train_decision_tree(leaf, class_column, features_ignore, use_IG))\n","    tree_current.leaves = list(leaves)\n","\n","    # print(\"Best feature: \", feature_best, \"Creating left tree\")\n","    # print(\"feature_best: \", feature_best, \" Threshold_best: \", threshold_best)\n","    # tree_current.left = train_decision_tree(data[data[:,feature_best] < threshold_best], class_column, features_ignore, use_IG)\n","\n","    # print(\"Best feature: \", feature_best,\"features_ignore: \", features_ignore, \"Creating right tree\")\n","    # tree_current.right = train_decision_tree(data[data[:,feature_best] >= threshold_best], class_column, features_ignore, use_IG)\n","  return tree_current\n","\n","\n","\n","def data_split(dataSamples, feature, thres):\n","  if thres is None:\n","    dataLeaves = []\n","    vals = np.unique((dataSamples)[:, feature])\n","    for val in vals:\n","      dataLeaves.append(dataSamples[dataSamples[:, feature] == val])\n","  else:\n","    leftData = dataSamples[dataSamples[:, feature] < thres]\n","    rightData = dataSamples[dataSamples[:, feature] >= thres]\n","    dataLeaves = [leftData, rightData]\n","  dataLeaves = np.asarray(dataLeaves)\n","  return dataLeaves\n","\n","\n","def get_entropy(y):\n","  \"\"\"Return the entropy of the data.\n","\n","  Parameters: \n","  y (np.array): np.array with the labels of the samples in the set\n","\n","  Returns: \n","  Entropy of the set \n","  \"\"\"\n","  labels, label_count = np.unique((y), return_counts=1)\n","\n","  total_samples = sum(label_count)\n","  entro = 0.\n","\n","  # Iterate over labels\n","  for num in label_count:\n","    # print(\"num: \", num, \" total samples \", total_samples)\n","    entro = entro + (num/total_samples * np.log2(num/total_samples))\n","    # print (\"entro\", entro)\n","\n","  entro = -entro\n","  return entro\n","\n","# Get conditional entropy of numerical features based on a threshold for the split of the data\n","def conditional_entropy_numerical(data, class_column, feature, threshold):\n","  \"\"\"Get conditional entropy of numerical features based on a threshold for the split of the data\n","  \"\"\"\n","  cond_entropy = 0.0\n","\n","  data_split = data[data[:, feature] < threshold]\n","  p = data_split.shape[0]/data.shape[0]\n","  y = data_split[:,class_column]\n","  cond_entropy = cond_entropy + get_entropy(y) * p\n","\n","  data_split = data[data[:, feature] >= threshold]\n","  p = data_split.shape[0]/data.shape[0]\n","  y = data_split[:,class_column]\n","  cond_entropy = cond_entropy + get_entropy(y) * p\n","\n","  return cond_entropy\n","\n","\n","def conditional_entropy_categorical(data, class_column, feature):\n","  \"\"\"Get conditional entropy of categorical features\n","  \"\"\"\n","  cond_entropy = 0.0\n","  feature_data = data[:, feature]\n","  categories, category_count = np.unique((feature_data), return_counts=1)\n","\n","  for category in categories:\n","    data_split = data[data[:, feature] == category]\n","    p = data_split.shape[0]/data.shape[0]\n","    y = data_split[:,class_column]\n","    cond_entropy = cond_entropy + get_entropy(y) * p\n","\n","  return cond_entropy\n","\n","# Get split information of numerical features based on a threshold for the split of the data\n","def split_information_numerical(data, feature, threshold):\n","  \"\"\"Get split information of numerical features based on a threshold for the split of the data\n","  \"\"\"\n","  split_info = 0.0\n","\n","  data_split = data[data[:, feature] < threshold]\n","  p = data_split.shape[0]/data.shape[0]\n","  split_info = split_info + (p * np.log2(p))\n","\n","  data_split = data[data[:, feature] >= threshold]\n","  p = data_split.shape[0]/data.shape[0]\n","  split_info = split_info + (p * np.log2(p))\n","\n","  split_info = -split_info\n","\n","  return split_info\n","\n","\n","def split_information_categorical(data, feature):\n","  split_info = 0.0\n","  feature_data = data[:, feature]\n","  categories, category_count = np.unique((feature_data), return_counts=1)\n","\n","  for category in categories:\n","    data_split = data[data[:, feature] == category]\n","    p = data_split.shape[0]/data.shape[0]\n","    split_info = split_info + (p * np.log2(p))\n","\n","  split_info = -split_info\n","  \n","  return split_info\n","\n","\n","\n","# Inspired in https://stackoverflow.com/a/16908217/12894766\n","def is_numerical(item):\n","  try:\n","    float(item)\n","    return True\n","  except:\n","    return False\n","\n","def best_conditional_entropy(data, class_column, features, IG):\n","  \"\"\"Return the entropy of the data.\n","\n","  Parameters: \n","  y (np.array): np.array with the labels of the samples in the set\n","\n","  Returns: \n","  Entropy of the set \n","  \"\"\"\n","  if verbose:\n","    print(\"Inside best_conditional_entropy\")\n","\n","  best_cond_ent = None\n","  best_feature = None\n","  best_thres = 0.\n","  # print(\"data left: {1:d},  Attributes left: {0:d}\".format(len(features), data.shape[0]))\n","  \n","  for feature in features:\n","    data = data[data[:, feature].argsort()] \n","    X = data[:, feature] \n","    y = data[:, class_column]\n","    labels_entropy = get_entropy(y)\n","    # print(\"System entropy: {1:.2f}, data left: {2:d},  Attributes left: {0:d}\".format(len(features), labels_entropy, data.shape[0]))\n","    # labels, label_count = np.unique((y), return_counts=1)\n","    # print(\"Labels: {0}, label_count: {1}, y[0] {2}\".format(labels, label_count, y[0]))\n","    # total_samples = sum(label_count)\n","    \n","\n","    # If the data is numerical:\n","    if all([is_numerical(item) for item in data[:, feature]]):\n","   \n","      # Check the possible thresholds\n","      possible_thresholds = []\n","      # print(y)\n","      label = y[0]\n","      for i in range(X.shape[0]):\n","        if label != y[i]: # Check if the labels are different\n","          #if they are, store the index\n","          possible_thresholds = np.append(possible_thresholds, X[i])\n","          label = y[i]\n","\n","      # prob_label = label_count/total_samples\n","\n","      for threshold in possible_thresholds:\n","                \n","        # Solved using Information Gain. We will study the conditional entropy as \n","        # the label entropy is the same for all features at this stage\n","        # Lower conditional entropy is best\n","\n","        if IG:\n","          conditional_ent = conditional_entropy_numerical(data, class_column, feature, threshold)\n","          # print(\"conditional_ent {0}\".format(conditional_ent))\n","        else:\n","          # conditional_ent = conditional_entropy_numerical(data, class_column, feature, threshold) / split_information_numerical(data, feature, threshold)\n","          # X_bool = np.array([x < threshold for x in X])\n","\n","          intrinsic_value = get_entropy(X<threshold)\n","          if intrinsic_value == -0.0:\n","            conditional_ent = np.inf\n","          else:\n","            conditional_ent = conditional_entropy_numerical(data, class_column, feature, threshold) / intrinsic_value\n","          # print(\"get_entropy boolean: \", get_entropy(X<threshold))\n","\n","        if (best_cond_ent == None or conditional_ent < best_cond_ent):\n","          # print(\"current_cond_ent: {0}, Better_cond_ent: {1}\".format(best_cond_ent,conditional_ent))\n","          # best_cond_ent = conditional_ent\n","          \n","          best_cond_ent = conditional_ent\n","          best_thres = threshold\n","          best_feature = feature\n","          is_categorical = False \n","          # print(\"best_cond_ent:{0}, cond_ent:{1}, best_feature:{2}, thres:{3}\".\n","                # format(best_cond_ent, conditional_ent, best_feature, best_thres))\n","      # print(\"feature {0}, best_threshold {1}\".format(best_feature, best_thres))\n","\n","    # If data is not numerical (categorical)\n","    else:\n","      if (IG):\n","        conditional_ent = conditional_entropy_categorical(data, class_column, feature)\n","      else:\n","        intrinsic_value = get_entropy(X)\n","        if intrinsic_value == -0.0:\n","            conditional_ent = np.inf\n","        else:\n","          conditional_ent = conditional_entropy_categorical(data, class_column, feature) / intrinsic_value\n","\n","      # print(\"conditional_ent {0}\".format(conditional_ent))\n","\n","      if (best_cond_ent == None or conditional_ent < best_cond_ent):\n","        # best_cond_ent = conditional_ent\n","        \n","        best_cond_ent = conditional_ent\n","        best_thres = np.unique(X)\n","        best_feature = feature\n","        is_categorical = True\n","      # print(\"Not numerical\")\n","\n","  return best_feature, best_thres, is_categorical\n","  \n"," \n","\n","  \n","\n","## Cross Validation\n","\n","def predict_point(tree, point):\n","  \"\"\"Return the prediction of a vector given a tree\n","\n","  Parameters: \n","  tree: tree model to use for the prediciton\n","  point: vector of features to make the prediction\n","\n","  Returns: \n","  label prediciton of a point\n","  \"\"\"\n","\n","  if (tree.is_leaf):\n","    return tree.prediction\n","  i = tree.feature\n","  print( \"tree.feature: \", i)\n","\n","  if tree.is_feature_categorical:\n","    categories = tree.categories\n","    \n","    # verbose\n","    # print(\"point[i]: {0}\\t categories: {1}\".format(point[i],categories))\n","    if point[i] in categories:\n","      category = np.where(categories == point[i])\n","\n","      # # verbose\n","      # print(\"categor: \", category)\n","      # print(\"category[]: \", category[0])\n","      # print(\"category index:\", int(category[0]))\n","      return predict_point(tree.leaves[int(category[0])], point)\n","    else:\n","      return tree.prediction\n","\n","    # if i in tree.leaves:\n","    #   return predict(tree.leaves[i], point)\n","    # else:\n","    #   return tree.prediction\n","  else:\n","    if (point[i] < tree.threshold):\n","      return (predict_point(tree.leaves[0], point))\n","    else:\n","      return (predict_point(tree.leaves[1], point))\n","\n","def predict_matrix(tree, data):\n","  \"\"\"Return the prediction of a matrix given a tree\n","\n","  Parameters: \n","  tree: tree model to use for the prediciton\n","  data: samplesto make the prediction\n","\n","  Returns: \n","  vector with prediciton of samples\n","  \"\"\"\n","  prediction = []\n","  for i in range(data.shape[0]):\n","    prediction.append(predict_point(tree, data[:,i]))\n","  return prediction\n","\n","\n","\n","def grouper (data, folds = 10):\n","  \"\"\"Returns the data with an assigned group number.\n","\n","  Parameters: \n","  data: data to be altered\n","  folds = number of groups to assign to the data\n","\n","  Returns: \n","  data with an assigned group number\n","  \"\"\"\n","  data_size = data.shape[0]\n","\n","  group = [*range(folds)]\n","  group = group*(int(np.floor(data_size/folds)))\n","  group = group + [*range(data_size%folds)]\n","\n","  # df = pd.DataFrame(data)\n","  # df['group']=np.array(group)\n","  data = np.c_[data, group]\n","  return data\n","\n","def ConfusionMatrix(real, predicted):\n","  classification_matrix = np.c_[real, predicted]\n","\n","  labels = np.unique(real)\n","  confusionMatrix = np.zeros((labels.shape[0],labels.shape[0]))\n","  accuracy = np.full(labels.shape[0],42)\n","  num_class_sample = np.full(labels.shape[0],42)\n","\n","  for i in range(labels.shape[0]):\n","    # j=i+1\n","    for j in range(labels.shape[0]):\n","      # l = k+1\n","      confusionMatrix[i,j] = real[real == labels[i]]\n","      confusionMatrix[i,j] = classification_matrix[classification_matrix[:,0]==labels[i]][classification_matrix[classification_matrix[:,0]==labels[i]][:,1]==labels[j]].shape[0]\n","      num_class_sample[j] = num_class_sample[j] + confusionMatrix[i,j]\n","      if i == j:\n","        accuracy[j] = accuracy[j] + confusionMatrix[i,j]\n","\n","  accuracy = np.round(np.divide(accuracy, num_class_sample),2)*100  \n","    \n","  return confusionMatrix, accuracy, num_class_sample\n","\n","def k_fold_validation(data, folds = 10, use_IG = True):\n","  \"\"\"Cross validation\n","\n","  Parameters: \n","  data: data to be altered\n","  folds = number of groups to split the data into\n","  Use_IG = parameter to control the training of the tree. True: Use Information gain. False: Use gain ratio\n","\n","  Returns: \n","  data with an assigned group number\n","  \"\"\"\n","  data_group = grouper(data, folds)\n","  groups = [x for x in range(folds)]\n","  best_tree = None\n","  prediction = []\n","  for group in groups:\n","    data_train = data_group[data_group[:,-1] != group]\n","\n","    data_train = np.delete(data_train, -1, 1)\n","\n","\n","    data_test = data_group[data_group[:,-1] == group]\n","    data_test = np.delete(data_test,-1, 1)\n","    print(\"data_test:\",data_test.shape)\n","    testX = data_test[:,:-1]\n","    testy = data_test[:,-1]\n","\n","    tree = train_decision_tree(data_train, use_IG)\n","\n","    prediction = predict_matrix(tree, testX)\n","\n","    # confusion_matrix = ConfusionMatrix(data_test, prediction)\n","    return prediction"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"s03Ez3IZx22p","colab_type":"code","colab":{}},"source":["tree_wine = train_decision_tree(np_wine, use_IG = True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FMMANkjoKNIB","colab_type":"code","colab":{}},"source":["# data = np_wine\n","# # np_wine.shape[1]\n","# class_column = data.shape[1]-1\n","# X = np.delete(data, class_column, axis=1) \n","# y = data[:, class_column]\n","# y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gX-VTtv0TTbn","colab_type":"code","colab":{}},"source":["tree_ttt = train_decision_tree(np_ttt, -1, use_IG= False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"q54uhMXJGg-1","outputId":"cef02954-1292-43e6-c418-c723ffb71ada","executionInfo":{"status":"ok","timestamp":1584503097128,"user_tz":240,"elapsed":329,"user":{"displayName":"Natalia Hoyos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghcs5ZP7rKTAzT6Gwfv3E0um_onlcGkkBv1PxLVajI=s64","userId":"09348541312897718275"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["predict_point(tree_ttt, np_ttt[332])\n","np_ttt.shape"],"execution_count":61,"outputs":[{"output_type":"stream","text":["tree.feature:  4\n","tree.feature:  0\n","tree.feature:  8\n","tree.feature:  1\n","tree.feature:  7\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(958, 10)"]},"metadata":{"tags":[]},"execution_count":61}]},{"cell_type":"code","metadata":{"id":"rtLxm_tl8vLJ","colab_type":"code","outputId":"18733b84-0076-4261-e924-1999ad2f2442","executionInfo":{"status":"error","timestamp":1584503152517,"user_tz":240,"elapsed":1807,"user":{"displayName":"Natalia Hoyos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghcs5ZP7rKTAzT6Gwfv3E0um_onlcGkkBv1PxLVajI=s64","userId":"09348541312897718275"}},"colab":{"base_uri":"https://localhost:8080/","height":683}},"source":["print(np_wine.size)\n","k_fold_validation(np_ttt)\n","# np_ttt"],"execution_count":64,"outputs":[{"output_type":"stream","text":["2492\n","data_test: (96, 10)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:64: FutureWarning: in the future insert will treat boolean arrays and array-likes as boolean index instead of casting it to integer\n"],"name":"stderr"},{"output_type":"stream","text":["tree.feature:  9\n","tree.feature:  9\n","tree.feature:  9\n","tree.feature:  9\n","tree.feature:  9\n","tree.feature:  9\n","tree.feature:  9\n","tree.feature:  9\n","tree.feature:  9\n"],"name":"stdout"},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-64-12dc2a7143ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_wine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mk_fold_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_ttt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# np_ttt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-63-5b186dafe884>\u001b[0m in \u001b[0;36mk_fold_validation\u001b[0;34m(data, folds, use_IG)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_decision_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_IG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;31m# confusion_matrix = ConfusionMatrix(data_test, prediction)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-63-5b186dafe884>\u001b[0m in \u001b[0;36mpredict_matrix\u001b[0;34m(tree, data)\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index 9 is out of bounds for axis 1 with size 9"]}]},{"cell_type":"code","metadata":{"id":"jy94gV_8wAkk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"553a96c4-c7b1-4436-d737-e958594251c6","executionInfo":{"status":"ok","timestamp":1584503342418,"user_tz":240,"elapsed":315,"user":{"displayName":"Natalia Hoyos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghcs5ZP7rKTAzT6Gwfv3E0um_onlcGkkBv1PxLVajI=s64","userId":"09348541312897718275"}}},"source":["data = np_ttt\n","folds = 10\n","data_group = grouper(data, folds)\n","data_test = data_group[data_group[:,-1] == 0]\n","data_test = np.delete(data_test,-1, 1)\n","# testX = data_test[:,:-1]\n","# testX.shape\n","data_test.shape"],"execution_count":71,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(96, 10)"]},"metadata":{"tags":[]},"execution_count":71}]},{"cell_type":"code","metadata":{"id":"sHRvKRHXz0FS","colab_type":"code","outputId":"449c17c9-0a3c-42c4-f606-7637ebe3a00f","executionInfo":{"status":"ok","timestamp":1584501126360,"user_tz":240,"elapsed":226,"user":{"displayName":"Natalia Hoyos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghcs5ZP7rKTAzT6Gwfv3E0um_onlcGkkBv1PxLVajI=s64","userId":"09348541312897718275"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["t = np.array([[1,2,3],[4,5,6]])\n","te = np.array([[1,1,2],[4,1,1]])\n","# y = t\n","# t = np.delete(t, -1, 1)\n","# t.size\n","# te = t[t[:,0]!=1]\n","t[t==1]"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 4])"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"ohVQAmulz9AG","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mww9c187DH25","colab_type":"code","colab":{}},"source":["y = np.array([[1,5,3,2,3],[3,5,4,2,3],[1,5,3,2,3],[1,5,3,2,3],[1,5,3,2,3]])\n","ya = np.array([1,5,3,2,3])\n","x = np.array([1,2,1,1,1])\n","len(np.unique(y))\n","# statistics.mode(y)\n","a,b = np.unique((np_wine)[:, 0], return_counts=1)\n","\n","a,b = np.unique(y, return_counts=1)\n","# max(set(y), key=y.tolist().count)\n","# y.tolist().count(3)\n","sum(b)\n","# y.shape[0]\n","np.log2(2)\n","get_entropy(y)\n","y[x>1]\n","z = np.delete(y, 1, axis=1)\n","yb = np.append(ya,x)\n","yb = np.append(yb,x)\n","y[1,2]\n","y/2\n","label = 1\n","get_entropy(ya[ya==label])\n","\n","y[1][y[1]==3]\n","# print(a,b)\n","\n","n = None\n","if (n == None or n>2):\n","  n = 1.2\n","  print(n)\n","a = np.unique((np_wine)[:, 0])\n","np.where(a == 2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sU--h5qPdMWO","colab_type":"code","colab":{}},"source":["None == 1"],"execution_count":0,"outputs":[]}]}